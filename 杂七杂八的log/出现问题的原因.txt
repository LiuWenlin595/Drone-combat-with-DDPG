步数太多，之前可以获得回报但是后来回报变少    对  可以理解为负样本太多了
惩罚太大，一直得不到奖励   对  负样本太多了
学习率太低，学的太慢了   这个有待考究
正负样本不一样多   负样本太多了，没办法  刚开始探索的时候就这样，存在一定的稀疏奖励问题
随机初始化动作，  # 没有办法，因为动作是由actor决定的
问题简单而网络太复杂，神经网络训练效果不好，层数多了容易过拟合梯度消失梯度爆炸啥的     改了网络结构，结果有待观察

数据之间相关性太强  replay buffer也没起到作用？
收敛于局部最大值？
当序列长度大于40时，传统的强化学习算法就算有各种探索机制的加持，也不能学会解决这个问题，
因为这个问题完全不是缺乏探索，而是状态太多，探索不完，导致奖励极其稀疏，算法根本不知道需要优化的目标在哪里。

输入归一化
先拿师兄的策略过来训练一下
把激活函数换成sigmoid  非线性更强一些
L1正则化  L2正则化


代码不同之处
ep_r在done之后是否应该归零
env的奖励设置还没看
DDPG的整体架构还没看