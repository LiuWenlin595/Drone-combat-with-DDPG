状态改为所有导弹的绝对位置，但是自己导弹的位置要放在最前面
Actor采用分布式的，只需要输入自己的状态就可以了，为了简单计算需要把网络设置的简单一点
Critic采用集中式的，每次输入所有导弹的状态，和这个导弹执行的动作，可以把网络设置的复杂一些
是否需要一个向量来标识  是哪个导弹

actor根据直接策略搜索方法的不同，可以采取两种策略
对于离散空间的动作，网络的输出格式可以是每个动作发生的可能性(sum=1)
对应的损失函数就是 - log(prob) * vt
对于连续空间的动作，网络的输出格式可以直接是对应的动作(size=1)
对应的损失函数就直接是 - vt (猜)
vt也有可能是td损失

TDerror大并不是说明这个动作不好，而是Critic估计的有问题，惊喜度高，需要更多的出现来使得估计准确


8.26学习内容：
做了一道leetcode题，将动作空间和状态空间的维度可扩充性
更改了Actor和Critic的网络大小，了解了batchnorm的使用位置以及γβ参数
读明白了Actor和Critic的更新模式，Critic取消了动作作为输入

下一步搞定DDPG部分的代码，主要是两个网络的损失函数


8.28学习内容
了解了detach, backward的工作原理
把Critic的输入又改成了Q值的形式
了解了确定性动作和非确定性动作求TDerror形式的不同(一个求V值一个求Q值)
下一步去Adam优化器里面看看具体是怎么优化的，为什么反向传播的梯度总是0

8.29学习内容
对actor_loss的更新有了更成熟的理解, 更新的是self.actor(state)而并不是action
反向传播部分都理解完了以后就可以开始训练了