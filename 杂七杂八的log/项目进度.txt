8.xx
舰艇可移动 随即飞 遇到边界就弹一下
导弹和船的位置随机初始化
 
9.8 
调整了右方无人机和敌方无人机的速度
防止了双方无人机的死亡，将越界的dxdy设定成了边界值
把总的无人机距离之和改成离他最近的无人机的距离
把角度的变化量改成确定角度
把状态中的速度信息去掉
每个改成以目标actor为中心的绝对位置输出 
导弹不会死 因此没法done  只能等到达最大轨迹长度

9.9
actor三次输出三个值再汇总起来，但采用同一个actor
代码可以跑了，效果奇差

9.11
Critic输入的state和actor输入的state一样，都采用局部信息作为输入
把样本轨迹拆成局部的三份输入buffer
更改env使其reward拆成三份

9.14
我自己的代码，drone.distance初始化是0，会白白得到一个惩罚
研究了师兄的代码，发现那个距离的回报在远处没啥用，在近处有用，暂时先放在服务器上跑了

9.18
改了回报，加了很多奖励，减少了很多惩罚
distance设计了新的函数，分子分母都考虑到了
网络结构改了一下，看起来更合理了

9.26
写个时间函数判断跑的慢的原因，没发现什么异常，整体都慢
简化了网络结构，怕网络太深训练不好  1 128 64 1

10.4
又加了一层网络，激活函数改成了elu   1 32 64 32 1
对于输入进行了bn
训练步长增加到了50W
Critic是一定要bn一下的，因为state和action的数据维度差的很大

10.13
发现了ndarray浅拷贝的重大bug
又训练了几个模型，初始网络来一个，复杂网络来一个，kunkun版超参配合2pi来一个，混合奖励来一个，复杂奖励来一个
Critic中的actor_dim和critic_dim是否需要用两个网络先训练一下再合并

10.30
两个周之前就能跑出来了，原因一是那个浅拷贝的bug，二是网络太简单，无法学习
但是虽然跑出来了，还是出现了三个追一个的问题
我又分别跑了韩坤DDPG-韩坤env，我DDPG-韩坤env，我DDPG-我env，发现我的奖励比他们大很多，回头找找原因


todo： 
输出网络值看一下， 
网络模型再稍微复杂一下
增大batchsize


todo：
tanh可能需要更改
回报因为所有无人机的汇总所以效果差

如果存在速度的话，可能存在弧线飞行
如果不存在速度的话，同一个状态同一个动作并不能唯一的确定下一个状态




